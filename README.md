

# Predicting Salary Project

## ğŸ“˜ Overview

This project focuses on **predicting job salaries** using **neural networks** that combine **categorical** and **textual (NLP)** features.
The dataset used comes from Kaggle:
ğŸ‘‰ [Job Salary Prediction Dataset](https://www.kaggle.com/c/job-salary-prediction/data)

The goal is to build and compare multiple models that process structured and unstructured data to achieve accurate salary predictions.

---

## ğŸ§  Project Objectives

1. Conduct a short **Exploratory Data Analysis (EDA)** to understand dataset structure and key relationships.
2. Build **baseline and advanced neural network models** using categorical and text-based features.
3. Experiment with **different text representation techniques** (TF-IDF, embeddings, pretrained models).
4. Optimize performance using **custom training loops**, **early stopping**, and **hyperparameter tuning**.
5. Focus mainly on **RMSE** as the key evaluation metric, while keeping flexibility to compute others.

---

## ğŸ“Š 1. Exploratory Data Analysis (EDA)

* Examined each columnâ€™s meaning, value ranges, and distributions.
* Conducted univariate and bivariate visualizations using `matplotlib` and `seaborn`.
* Checked for missing values and outliers.
* Basic text preprocessing overview for description and title columns.

---

## ğŸ§© 2. Modeling Approach

### **Part 1: Baseline Model (Categorical Features Only)**

* Built preprocessing pipelines for categorical data (encoding, scaling, imputing).
* Created a **custom PyTorch dataset** and **feedforward neural network**.
* Implemented **custom early stopping** and **RMSE-based evaluation**.
* Experimented with hyperparameters such as learning rate, hidden layer sizes, batch size, and dropout.

---

### **Part 2: Text Integration**

Text columns added:

* **Full Description**
* **Job Title**

#### ğŸ”¹ Text Representation Techniques

1. **TF-IDF Vectorization** + optional dimensionality reduction using **Truncated SVD**.
2. **Sentence Embeddings** via **Sentence Transformers**.

* Created pipelines for text preprocessing and vectorization.
* Built models with **multi-input architectures** combining categorical and text data.
* Compared models using **scaled** and **unscaled** target values.

#### ğŸ”¹ Self-Taught & Pretrained Embeddings

* Trained a **custom Word2Vec model** on the dataset to learn domain-specific embeddings.

* Used **pretrained FastText embeddings** to capture semantic relationships.

* Manually **tokenized text data**, built a **vocabulary**, and **included the embedding matrix directly in the neural network**.

* Explored three embedding integration strategies:

  1. Using **precomputed embeddings** as direct model input.
  2. Using a **frozen embedding layer** initialized with pretrained weights.
  3. Using a **trainable embedding layer** for fine-tuning.

* Tested multiple architectures:

  * Multi-input simple networks
  * CNN-based networks
  * Networks with residual connections
  * Combinations of the above

* Applied regularization, dropout, and normalization to reduce overfitting.

* Visualized and compared model performance curves.

---

## ğŸ§ª Training & Evaluation

* Implemented a **custom training loop** using PyTorch.
* Main metric: **Root Mean Squared Error (RMSE)**.
* Additional metrics available: MAE
* Used **ReduceLROnPlateau** scheduler for dynamic learning rate adjustment.
* Logged training progress and visualized loss and validation curves.

---

## âš™ï¸ Technologies Used

### **Programming & Frameworks**

* **Python 3.12** (recommended â‰¤ 3.12 for gensim compatibility)
* **PyTorch** â€“ neural network modeling and training
* **scikit-learn** â€“ preprocessing, pipelines, and evaluation
* **gensim** â€“ Word2Vec and FastText embeddings
* **sentence-transformers** â€“ sentence-level embeddings
* **category_encoders** â€“ target encoding for categorical variables
* **joblib** â€“ saving and loading models

### **Data & NLP Tools**

* **pandas**, **NumPy** â€“ data handling and transformations
* **NLTK** â€“ tokenization, lemmatization, stopword removal
* **TF-IDF**, **TruncatedSVD** â€“ feature extraction and dimensionality reduction

### **Visualization & Analysis**

* **matplotlib**, **seaborn** â€“ EDA and training visualization

---

## ğŸ“‚ Repository Structure

```
PROJECT2_NEURAL_NETWORK_PREDICTION/
â”œâ”€â”€ config/                               # Configuration files
â”‚   â””â”€â”€ config.py                         # Global constants and paths
â”‚
â”œâ”€â”€ data/                                 # Raw dataset
â”‚   â””â”€â”€ Train_rev1.csv                    # Main training dataset
â”‚
â”œâ”€â”€ embeddings/                           # Pre-trained or learned word embeddings
â”‚
â”œâ”€â”€ preprocessors/                        # Saved preprocessing objects 
â”‚
â”œâ”€â”€ tfidf_features/                       # Stored TF-IDF features
â”‚
â”œâ”€â”€ models/                               # Saved trained models 
â”‚
â”œâ”€â”€ notebooks/                            # Jupyter notebooks
â”‚   â””â”€â”€ nlp_salary_prediction_project.ipynb  # Main analysis and modeling notebook
â”‚
â”œâ”€â”€ reports/                              # Reports and visual results
â”‚   â”œâ”€â”€ figures/                          # Generated plots and charts
â”‚   â”œâ”€â”€ all_models_results.csv             # Summary of all model evaluations
â”‚   â”œâ”€â”€ project_report.md                 # Detailed project report
â”‚   â””â”€â”€ model_naming.md                   # Documentation for model naming conventions
â”‚
â”œâ”€â”€ src/                                  # Core source code
â”‚   â”œâ”€â”€ data_preprocessing/               # Data loading and preprocessing scripts
â”‚   â”‚   â”œâ”€â”€ loader.py                     # Loads raw data
â”‚   â”‚   â”œâ”€â”€ preprocess_categorical.py     # Encodes categorical features
â”‚   â”‚   â”œâ”€â”€ preprocess_target.py          # Preprocesses target variable (salary)
â”‚   â”‚   â”œâ”€â”€ splitter.py                   # Splits dataset into train/validation/test
â”‚   â”‚   â””â”€â”€ text_features.py              # Extracts textual features
â”‚   â”‚
â”‚   â”œâ”€â”€ datasets/                         # Custom dataset wrappers for PyTorch 
â”‚   â”‚   â”œâ”€â”€ multi_input_dataset.py
â”‚   â”‚   â”œâ”€â”€ pre_encoded_dataset.py
â”‚   â”‚   â”œâ”€â”€ salary_dataset.py
â”‚   â”‚   â””â”€â”€ tokens_dataset.py
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                           # Neural network architectures
â”‚   â”‚   â”œâ”€â”€ embedding_matrix_model.py
â”‚   â”‚   â”œâ”€â”€ integrated_model.py
â”‚   â”‚   â”œâ”€â”€ multi_input_model.py
â”‚   â”‚   â”œâ”€â”€ pre_encoded_model.py
â”‚   â”‚   â”œâ”€â”€ self_taught_models.py
â”‚   â”‚   â”œâ”€â”€ simple_regressors.py
â”‚   â”‚   â”œâ”€â”€ unfrozen_models.py
â”‚   â”‚   â””â”€â”€ residual_block.py
â”‚   â”‚
â”‚   â”œâ”€â”€ pipeline/                         # Feature pipeline assembly
â”‚   â”‚   â””â”€â”€ feature_pipeline.py
â”‚   â”‚
â”‚   â”œâ”€â”€ preprocessors/                    # NLP preprocessing utilities
â”‚   â”‚   â”œâ”€â”€ pretrained_utils.py
â”‚   â”‚   â”œâ”€â”€ text_embedder.py
â”‚   â”‚   â”œâ”€â”€ text_preprocessors.py
â”‚   â”‚   â”œâ”€â”€ tfidf_transformer.py
â”‚   â”‚   â”œâ”€â”€ vocabulary.py
â”‚   â”‚   â””â”€â”€ word2vec_utils.py
â”‚   â”‚
â”‚   â”œâ”€â”€ training/                         # Training and evaluation logic
â”‚   â”‚   â”œâ”€â”€ early_stopping.py
â”‚   â”‚   â”œâ”€â”€ evaluate_model.py
â”‚   â”‚   â””â”€â”€ train_model.py
â”‚   â”‚
â”‚   â””â”€â”€ utils/                            # Helper functions
â”‚       â”œâ”€â”€ device_utils.py               # Device (MPS/CPU/GPU) handling
â”‚       â”œâ”€â”€ logging_utils.py              # Logging configuration
â”‚       â”œâ”€â”€ metrics_utils.py              # Custom evaluation metrics
â”‚       â”œâ”€â”€ plot_utils.py                 # Plotting utilities
â”‚       â””â”€â”€ seed_utils.py                 # Reproducibility (random seeds)
â”‚   
â”‚
â”œâ”€â”€ requirements.txt                      # Project dependencies
â”œâ”€â”€ orchestrator.py                       # Main pipeline orchestrator (runs full workflow)
â”œâ”€â”€ README.md                             # Project overview
â””â”€â”€ .gitignore                            # Ignored files and directories for Git

```
---

## ğŸ“ˆ Results

The detailed **analysis of results** and performance comparison of all models are presented in
**`reports/project_report.md`**.

---


